# Dual Diffusion
Dual Diffusion is a generative diffusion model for music. This model and the code in this repository is still a work-in-progress.

I started this project in August/2023 with the intention of achieving 2 goals:
* Familiarize myself with every component of modern diffusion models
* Produce a model able to generate music that I would actually want to listen to
* Do this using only a single consumer GPU (4090)

The model has changed substantially over the course of development.
* Initially (August/2023) the diffusion model worked directly on raw audio. Due to memory and performance constraints this meant I was limited to ~16 seconds of mono audio at 8khz. I experimented with 1d and 2d formats with various preprocessing steps. I found 2d formats were able to generalize better with a small dataset and were more compute and parameter efficient than 1d formats. For 2d formats I also found using separable attention (merging the rows / columns with the batch dimension alternately) could make using attention in a high dimensionality model practical without sacrificing too much quality. I found that attention was an absolute requirement to allow the model to understand the perceptual ~log-frequency scale in music when the format used a linear-frequency scale. I also found that v-prediction with a cosine-based schedule worked significantly better than any alternatives.
* In December/2023 I started training variational auto-encoder models to try a latent diffusion model. I found that point-wise loss works very poorly in terms of reconstruction quality, and multi-scale spectral loss was a much better option. I found that multi-scale spectral power density loss works considerably better for music and learning pitch if you add an appropriately weighted loss term for phase. I found that although it was possible to train a VAE including the signal phase to have good reconstruction quality and a compact latent space, the latent space was not easily interpretable by the latent diffusion model.
* In March/2024 I started using mel-scale spectrogram based formats, excluding phase information. I found that it was possible to considerably improve the FGLA phase reconstruction by tuning the window and spectrogram parameters, as well as modifying the algorithm to anneal phases for stereo signals in a more coherent way. I settled on a set of parameters that resulted in a spectrogram dimensionality that is the same as the critically sampled raw audio without sacrificing too much perceptual quality.
* In April/2024 I replaced the diffusion model unet with the improved edm2 unet. I made several improvements to the edm2 unet: Replaced einsum attention with torch sdp attention, replaced fourier embedding frequencies / phases for a smoother inner product space, added class label dropout in a way that preserves magnitude on expectation, replaced the weight normalization in the forward method of the mpconv module with weight normalization that is only applied when the weights are updated, added correction for the output magnitude when using dropout inside blocks, replaced the upsample/downsample with equivilent torch built-ins, and merged some of the pre-conditioning code into the unet itself. I also started using torch dynamo / model compilation and added the appropriate compiler hints for maximum performance.
* In May/2024 I adopted the edm/ddim noise schedule, sampling algorithm, and learn rate schedule. I found that the log-normal sigma sampling in training could be improved by using the per-sigma estimated error variance to calculate a sigma sampling pdf that explicitly focuses on the noise levels that the model is most capable of making progress on. I found that using stratified sampling to distribute sigmas as evenly as possible within each mini-batch could mitigate problems with smaller mini-batches. I began pre-encoding the latents for my dataset before diffusion model training for increased performance and reduced memory consumption. I found pre-encoding the latents before random crop can negatively influence model quality due to lacking the variations created by sub-latent-pixel offsets if the encoding occurs post-random crop and added pre-encoded latent variations for sub-latent-pixel offsets.

